---
title: Self Organizing Map for WSE
description: Creating a Self Organizing Map for the Warsaw Stock Exchange and using financial & economic data as explanatory variables
author: Pawel Sobala
date: '2020-01-27'
slug: self-organizing-map-wse
categories: ["R", "SOM"]
tags: ["R Markdown", "Self Organizing Map", "Kohonen"]
#output: 
#  html_document:
#    keep_md: true
#    toc: true
#    toc_depth: 3
#    toc_float: 
#      collapsed: false
#      smooth_scroll: true
#    number_sections: true  ## if you want number sections at each table header
#    theme: united  # many options for theme, this one is my favorite.
#    highlight: tango  # specifies the syntax highlighting style - tango # https://www.garrickadenbuie.com/blog/pandoc-syntax-highlighting-examples/
---



<style>
body {
text-align: justify}
</style>
<blockquote>
<p>This is a first post from a series of articles on Self Organizing Maps (SOM). The next post will cover the topic regarding analyzing the SOM and its quality.</p>
</blockquote>
<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>The purpose of this short article is to create a Self Organizing Map (SOM) using the <a href="https://cran.r-project.org/web/packages/kohonen/index.html">Kohonen R package</a>, which could help understand the whole Warsaw Stock Exchange (<a href="https://www.gpw.pl/en-home">WSE</a>) market based on financial and economic data.
Moreover, in this article I write my simple own function using <code class="r">data.table</code>, <code class="r">purrr</code> and <code class="r">magrittr</code> R packages to efficiently download data from <a href="https://stooq.com">stooq</a>. Note that this analysis closely follows what is already presented <a href="https://github.com/KaroRonty/SelfOrganizingStockMarket">here</a> for the US stock market.</p>
<div id="self-organizing-maps" class="section level3">
<h3>Self Organizing Maps</h3>
<p>To get to know better or the basics of Self Organizing Maps, I recommend to read as an introduction <a href="https://en.wikipedia.org/wiki/Self-organizing_map">this Wikipedia article</a> and as an extension the article from the author of <a href="https://page.mi.fu-berlin.de/rojas/neural/chapter/K15.pdf">SOM algorithm Teuvo Kohonen</a>.</p>
</div>
</div>
<div id="building-the-self-organizing-map" class="section level1">
<h1>Building the Self Organizing Map</h1>
<div id="getting-data" class="section level2">
<h2>Getting Data</h2>
<p>In the previous blog post I tried various ways to download the financial data using function <code class="r">tq_get()</code> from the <code class="r">tidyquant</code> package. This time however I will utilize a slightly different approach. I will write my own function to download the data from the <a href="https://stooq.com">stooq</a> website, which uses data provided by <a href="https://www.vwd.com/home/">VWD</a>. Unfortunately, the stooq website is available only in polish language. Although the approach to get the data doesn’t require the knowledge of polish. The first step is to specify the tickers for the following financial/economic data:</p>
<ul>
<li>Warsaw Stock Exchange Index</li>
<li>price/equity ratio for the whole stock market</li>
<li>price/book value ratio for the whole stock market</li>
<li>dividend yield for the whole stock market</li>
<li>unemployment rate</li>
<li>consumer price index y/y</li>
<li>wibor pln 1y - interest rate</li>
<li>10y bond yield</li>
</ul>
<p>Please note that further in the R script and the article I will stick to the tickers mentioned in the following block of code.</p>
<pre class="r"><code>tickers &lt;- c(&quot;wig&quot;, #Warsaw Stock Exchange Index
             &quot;wig_pe&quot;, #price/equity ratio for the whole stock market
             &quot;WIG_PB&quot;, #price/book value ratio for the whole stock market
             &quot;wig_dy&quot;, #dividend yield for the whole stock market
             &quot;UNRTPL.M&quot;, #unemployment rate
             &quot;CPIYPL.M&quot;, #consumer price index y/y
             &quot;PLOPLN1Y&quot;, #intrest rate- wibor 1y
             &quot;10PLY.B&quot;) #10y bond yield</code></pre>
<p>To get data form stooq website I write a simple function <code>read_stooq_data</code> and then I apply it to all tickers using a <code class="r">map</code> function from the <code class="r">purrr</code> package. In this function I utilize the <code class="r">fread()</code> function from the <code>data.table</code> package to download as a data.table object contents of a web url. The frequency of data is set to daily.</p>
<pre class="r"><code>read_data_stooq &lt;- function(x){
  fread(paste0(&quot;https://stooq.pl/q/d/l/?s=&quot;,x,&quot;&amp;i=d&quot;), fill=TRUE) %&gt;% 
    select(Date=Data, !!x:=Zamkniecie)
} #&#39;zamkniecie&#39; means close value

data_all &lt;- purrr::map(tickers, read_data_stooq) </code></pre>
<p>Note that in the <code class="r">read_data_stooq</code> function I deliberately used double exclamation mark to pass each ticker name, as column name.</p>
</div>
<div id="manipulating-wrangling-data" class="section level2">
<h2>Manipulating &amp; Wrangling Data</h2>
<p>I have data on all tickers saved as data.tables within a list. It would be the best to merge those different elements of the list into a single data.table. For this purpose I use the following function downloaded from this <a href="https://www.brodrigues.co/blog/2016-07-30-merge-a-list-of-datasets-together/">blog</a>.</p>
<pre class="r"><code>multi_join &lt;- function(list_of_loaded_data, join_func, ...){
  output &lt;- Reduce(function(x, y) {join_func(x, y, ...)}, list_of_loaded_data)
}

merged_data &lt;- multi_join(data_all, full_join) 
merged_data_interpolate &lt;- merged_data %&gt;%
  # pivot_longer(cols=&quot;Date&quot;,values_to =&quot;series&quot; )
  reshape2::melt(id.vars = &#39;Date&#39;, variable.name = &#39;series&#39;)</code></pre>
<p>To make sure that no data is lost in the merge process I utilized the <code>full_join</code> option. Otherwise, as the frequency or the availability of data is different I could lose some important data. To work further with the <code>som_data</code> object I have to use the <code>na.approx()</code> function from the <code>zoo</code> package to fill all NA values.</p>
<pre class="r"><code>merged_data_interpolate &lt;- merged_data_interpolate %&gt;% 
  arrange(series, Date) %&gt;% 
  group_by(series) %&gt;% 
  mutate_at(vars(matches(&quot;value&quot;)), 
            list(value=na.approx), 
            na.rm=FALSE, 
            rule=1:2,
            method=&quot;linear&quot;)
 
merged_data &lt;- merged_data_interpolate %&gt;% 
  pivot_wider(names_from = series, values_from = value) %&gt;% 
  arrange(Date)

som_data &lt;- merged_data  %&gt;% 
  na.omit()</code></pre>
<p>To make sure that all NA values are properly approximated I use the option <code class="r">rule=1:2</code>. It describes how interpolation is taking place outside the interval [min(x), max(x)]. If rule is 1 then NAs are returned for such points and if it is 2, the value at the closest data extreme is used. Economic data on inflation or unemployment is published only once in a while. In periods in between there were NAs. To fill them I could use either a linear function, a spline or a step-function. I decided to go with a linear function. As a validation step I apply also the <code>na.omit()</code> function to delete any rows with at least one NA value.</p>
<p>Finally, the resulted data set starts on:</p>
<pre class="r"><code>som_data %&gt;% 
  slice(1) %&gt;% 
  pull(Date) %&gt;% 
  as.Date()</code></pre>
<pre><code>## [1] &quot;2007-10-19&quot;</code></pre>
<p>and ends on:</p>
<pre class="r"><code>som_data %&gt;% 
  slice(n()) %&gt;% 
  pull(Date) %&gt;% 
  as.Date()</code></pre>
<pre><code>## [1] &quot;2020-02-14&quot;</code></pre>
<p>The final step is to ensure that the data used for the SOM is scaled to account for the correctness of distance.</p>
<pre class="r"><code>som_data_no_date &lt;- som_data %&gt;% 
  select(-Date)

som_data_scaled &lt;- apply(som_data_no_date, 2, scale)</code></pre>
<div id="sanity-check" class="section level3">
<h3>Sanity check</h3>
<p>As a sanity check I plot all variables in the data set, to investigate whether there are clear outliers.</p>
<pre class="r"><code>df &lt;- reshape2::melt(som_data, id.vars = &#39;Date&#39;, variable.name = &#39;series&#39;)
#obviously, one could also utilize the pivot_longer() function
sanity_plots &lt;- ggplot(df, aes(Date,value,group = 1)) + 
  geom_line() + 
  facet_grid(series ~ ., scales = &quot;free&quot;)

# ggplotly(sanity_plots) %&gt;%
#   layout(legend = list(orientation = &quot;h&quot;, x = 0.4, y = -0.2))
# Plotly plot commented out, as it is weights ~3mb
sanity_plots</code></pre>
<p><img src="/blog/2020/2020-01-27-self-organizing-map-wse_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>No apparent errors are visible. Maybe only one a sharp decrease in <code class="r">wig_dy</code> variable around 2009 should be investigated.</p>
</div>
</div>
<div id="building-the-map" class="section level2">
<h2>Building the map</h2>
<p>Using <code class="r">kohonen</code> package is effortless. Firstly, the grid for the Self Organizing Map needs to be specified. A simple 5x5 hexagonal grid should work out. Neighbourhood function specifies how the distance between 2 neurons in any step should be calculated. <code class="r">rlen</code> specifies the number of updates. <code>alpha</code> is the monotonically decreasing learning parameter. Additionally, I’m setting a seed to ensure that this analysis is reproducible.</p>
<pre class="r"><code>set.seed(1029)
som_grid &lt;- somgrid(xdim = 5, ydim = 5, topo = &quot;hexagonal&quot;, neighbourhood.fct=&quot;gaussian&quot;)

som_model &lt;- som(som_data_scaled,
                 grid = som_grid,
                 rlen = 300,
                 alpha = c(0.05, 0.01),
                 keep.data = TRUE)</code></pre>
<p>The main point of any SOM analysis is to produce a map. Of course a colorful map is more meaningful than a map in different shades of grey or any other color.</p>
<pre class="r"><code># Set heat colors palette
heatColors &lt;- function(n, alpha = 1) {
  rev(designer.colors(n = n, col = brewer.pal(9, &quot;Spectral&quot;)))
}</code></pre>
<p>Also a function for plotting SOM heat maps for each variable would be helpful.</p>
<pre class="r"><code>plot_som &lt;- function(variable) {
  unit_colors &lt;- aggregate(data.frame(som_data[, variable]),
                           by = list(som_model$unit.classif),
                           FUN = mean,
                           simplify = TRUE)
  
  plot(som_model,
       type = &quot;property&quot;,
       shape = &quot;straight&quot;,
       property = unit_colors[, 2],
       main = variable,
       palette.name = heatColors)
}</code></pre>
<p>Once properly equipped with tools it is possible to plot all variables that are categorized by a SOM.</p>
<p><img src="/blog/2020/2020-01-27-self-organizing-map-wse_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>The next step is to create the <code class="r">type="codes"</code> plot to understand what is the contribution of each variable to each cluster.</p>
<pre class="r"><code>plot(som_model,
     shape = &quot;straight&quot;,
     type = &quot;codes&quot;)</code></pre>
<p><img src="/blog/2020/2020-01-27-self-organizing-map-wse_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>Finally, the plotted clusters are presented. It is assumed that 4 clusters are created.</p>
<pre class="r"><code>palette &lt;- c(&quot;#F25F73&quot;, &quot;#98C94C&quot;, &quot;#888E94&quot;, &quot;#33A5BF&quot;, &quot;#F7D940&quot;)
som_cluster &lt;- cutree(hclust(dist(as.data.frame(som_model$codes))), 4)

plot(som_model,
     type = &quot;mapping&quot;,
     bgcol = palette[som_cluster],
     main = &quot;Clusters&quot;,
     shape = &quot;straight&quot;)</code></pre>
<p><img src="/blog/2020/2020-01-27-self-organizing-map-wse_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
</div>
</div>
<div id="summary" class="section level1">
<h1>Summary</h1>
<p>A simple Self Organizing Map helping to help explain the Warsaw Stock Exchange market was created in this article. In the next one it will be assessed in terms of the quality of map, as well as analysis findings.</p>
</div>
